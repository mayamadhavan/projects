{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mass Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mayamadhavan/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dateutil.parser\n",
    "from datetime import date\n",
    "\n",
    "from pprint import pprint\n",
    "import re\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import patsy\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping From Main Steam Search Page\n",
    "Able to Scrape:\n",
    "   1. Game Title (*DataFrame Index*)\n",
    "   2. Systems that the game runs on\n",
    "   3. Release date of game in days\n",
    "   4. Discount by numerical percent\n",
    "   5. Price of Game\n",
    "   6. Number of Reviews (**Dependent Variable**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "Function for parsing date elements in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def to_date(datestring):\n",
    "#     date = dateutil.parser.parse(datestring)\n",
    "#     return date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for creating list of links to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links=['https://store.steampowered.com/search/']\n",
    "# for i in range(1,1954):\n",
    "#     links.append('https://store.steampowered.com/search/?page='+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['title', 'link', 'system', 'systems', 'reviews', 'release_date', 'discount', 'price']\n",
    "# df=pd.DataFrame(columns=cols, index=[])\n",
    "\n",
    "# age=date.today()\n",
    "# for link in links:\n",
    "#     response=requests.get(link)\n",
    "#     soup = BeautifulSoup(response.text, 'html5lib')\n",
    "#     try: \n",
    "#         main_search_table=soup.find('div', id='search_result_container').find_all('div')[1]\n",
    "#     except:\n",
    "#         time.sleep(20)\n",
    "#         main_search_table=soup.find('div', id='search_result_container').find_all('div')[1]\n",
    "#     print(link)\n",
    "#     for game in main_search_table.find_all('a'):\n",
    "#         row={}\n",
    "#         row['title']=game.find('span', class_=\"title\").text #title\n",
    "#         row['link']=game['href'] #link\n",
    "#         systems=[system[\"class\"][1] for system in game.find('p').find_all('span')] #system\n",
    "#         row['system']=len(systems)\n",
    "#         row['systems']=systems\n",
    "#         try:\n",
    "#             row['release_date']=(age-(to_date(game.find('div', class_=\"col search_released responsive_secondrow\").text).date())).days #release date  \n",
    "#         except:\n",
    "#             row['release_date']=0\n",
    "#         for price in game.find_all('div', class_=\"col search_price_discount_combined responsive_secondrow\"): #price and discount\n",
    "#             for discount in price.find_all('div', class_=\"col search_discount responsive_secondrow\"):\n",
    "#                 if discount.find('span'):\n",
    "#                     row['discount']=abs(int(discount.find('span').text.strip('%')))\n",
    "#                     try:\n",
    "#                         row['price']=float(price.find('span', style=\"color: #888888;\").text.strip(\"$\"))\n",
    "#                     except:\n",
    "#                         price = re.sub('[^0-9,.]','', game.find('div', class_=\"col search_price responsive_secondrow\").text).replace(\"\\n\",\"\").strip(\"\\t\")\n",
    "#                         row['price']=price\n",
    "#                 else:\n",
    "#                     row['discount']=0\n",
    "#                     if not game.find('div', class_=\"col search_price responsive_secondrow\").text.replace(\"\\n\",\"\").strip(\"\\t\"):\n",
    "#                         row['price']=0\n",
    "#                     elif game.find('div', class_=\"col search_price responsive_secondrow\").text.replace(\"\\n\",\"\").strip(\"\\t\").lower()[0]!='$': #'free to play' or game.find('div', class_=\"col search_price responsive_secondrow\").text.replace(\"\\n\",\"\").strip(\"\\t\").lower()=='free':\n",
    "#                         row['price']=0\n",
    "#                     else:\n",
    "#                         row['price']=float(game.find('div', class_=\"col search_price responsive_secondrow\").text.replace(\"\\n\",\"\").strip(\"\\t\").strip(\"$\"))  \n",
    "#         for y in game.find_all('div',class_=\"col search_reviewscore responsive_secondrow\"):\n",
    "#             x=[review['data-tooltip-html'] for review in y.find_all('span')]\n",
    "#             if not x:\n",
    "#                 row['reviews']=int(0)\n",
    "#             else:\n",
    "#                 z=x[0].split('<br>')\n",
    "#                 d=z[1].split(\" \")\n",
    "#                 row['reviews']=int(d[3].replace(',', '')) #reviews\n",
    "#         df=df.append(row, ignore_index=True)\n",
    "#         with open('steam_search.pkl', 'wb') as picklefile:\n",
    "#             pickle.dump(df, picklefile)\n",
    "# #     time.sleep(3)\n",
    "    \n",
    "# df.set_index('title', inplace=True)\n",
    "# df.rename(columns=lambda x: x.strip())\n",
    "# pd.options.display.max_rows = 4000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make copy of df from pickle and read into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"steam_search.pkl\", 'rb') as picklefile: \n",
    "#       df2 = pickle.load(picklefile)\n",
    "# df2.to_csv('steam3.csv')\n",
    "# df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in csv from previous scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('steam3.csv')\n",
    "del(df['Unnamed: 0'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicates (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_df = df[~df.index.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying DataFrame Element Types to Support Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_df['system'] = no_dup_df.system.astype(int)\n",
    "no_dup_df['reviews'] = no_dup_df.reviews.astype(int)\n",
    "no_dup_df['release_date'] = no_dup_df.release_date.astype(int)\n",
    "no_dup_df['discount'] = no_dup_df.discount.astype(int)\n",
    "no_dup_df['price'] = no_dup_df.price.astype(float)\n",
    "no_dup_df['systems'] = no_dup_df.systems.astype(list)\n",
    "#no_dup_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Analysis (IA): DataFrame Resulting From Main Page Scrape \n",
    "### 1. Initial Look at Relationships with Pairplots and Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(no_dup_df, size = 1.2, aspect=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_dup_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(no_dup_df.corr(), cmap=\"seismic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split into Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=no_dup_df['reviews']\n",
    "X=no_dup_df.drop(['reviews','link'],1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "traindf=X_train.join(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run Patsy Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train, X_train = patsy.dmatrices('reviews ~ system + price + release_date + discount', data=traindf)\n",
    "model = sm.OLS(y_train, X_train)\n",
    "fit = model.fit()\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 10 Fold Cross-Validation Help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "scores = cross_val_score(lr, X_train, y_train, cv=10, scoring='mean_squared_error')\n",
    "print(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Step: Run Lasso Help and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est=Lasso(alpha=1e-5)\n",
    "est.fit(X_train,y_train)\n",
    "est.alpha\n",
    "zipped=zip(X.columns,est.coef_)\n",
    "features=sorted(zipped,key = lambda t: t[1],reverse=True)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and General DataFrame Mods\n",
    "\n",
    "### 1. Modify DF to Account for Skew by Logging Reviews; Repeat IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_df['log_reviews'] = np.log(no_dup_df.reviews + 1)\n",
    "#plt.hist(no_dup_df['log_reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(no_dup_df, size = 1.2, aspect=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(no_dup_df.corr(), cmap=\"seismic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1=no_dup_df['log_reviews']\n",
    "X1=no_dup_df.drop(['log_reviews','reviews','link'],1)\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.3)\n",
    "traindf1=X1_train.join(y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_train, X1_train = patsy.dmatrices('log_reviews ~ system + price + release_date + discount', data=traindf1)\n",
    "model1 = sm.OLS(y1_train, sm.add_constant(X1_train))\n",
    "fit1 = model1.fit()\n",
    "fit1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr1 = LinearRegression()\n",
    "scores = cross_val_score(lr1, X1_train, y1_train, cv=10, scoring='mean_squared_error')\n",
    "print(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Seperate Windows/Mac/Linux into Individual Bool 0/1 Columns; Repeat IA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows=[1 if 'win' in item else 0 for item in no_dup_df.systems]\n",
    "mac=[1 if 'mac' in item else 0 for item in no_dup_df.systems]\n",
    "linux=[1 if 'linux' in item else 0 for item in no_dup_df.systems]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_df['windows']=pd.Series(windows)\n",
    "no_dup_df['mac']=pd.Series(mac)\n",
    "no_dup_df['linux']=pd.Series(linux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_df['windows'] = no_dup_df.windows.astype(int)\n",
    "no_dup_df['mac'] = no_dup_df.mac.astype(int)\n",
    "no_dup_df['linux'] = no_dup_df.linux.astype(int)\n",
    "#no_dup_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(no_dup_df, size = 1.2, aspect=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(no_dup_df.corr(), cmap=\"seismic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2=no_dup_df['log_reviews']\n",
    "X2=no_dup_df.drop(['log_reviews','reviews','link'],1)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.3)\n",
    "traindf2=X2_train.join(y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_train, X2_train = patsy.dmatrices('log_reviews ~ system + price + release_date + discount + windows + mac + linux', data=traindf2)\n",
    "model2 = sm.OLS(y2_train, sm.add_constant(X2_train))\n",
    "fit2 = model2.fit()\n",
    "fit2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr2 = LinearRegression()\n",
    "scores = cross_val_score(lr2, X2_train, y2_train, cv=10, scoring='mean_squared_error')\n",
    "print(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Drop Movies;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies=[]\n",
    "for i in range(len(no_dup_df['systems'])):\n",
    "    element=(no_dup_df['systems'][i])\n",
    "    if 'win' not in element and 'mac' not in element and 'linux' not in element:\n",
    "        movies.append(1)\n",
    "    else:\n",
    "        movies.append(0)\n",
    "no_dup_df['movies']=movies\n",
    "no_dup_df = no_dup_df[no_dup_df.movies != 1]\n",
    "del(no_dup_df['movies'])\n",
    "no_dup_df.set_index('title', inplace=True)\n",
    "del(no_dup_df['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(no_dup_df, size = 1.2, aspect=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(no_dup_df.corr(), cmap=\"seismic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3=no_dup_df['log_reviews']\n",
    "X3=no_dup_df.drop(['log_reviews','reviews','link'],1)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.3)\n",
    "traindf3=X3_train.join(y3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3_train, X3_train = patsy.dmatrices('log_reviews ~ system + price + release_date + discount + windows + mac + linux', data=traindf3)\n",
    "model3 = sm.OLS(y3_train, sm.add_constant(X3_train))\n",
    "fit3 = model3.fit()\n",
    "fit3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr3 = LinearRegression()\n",
    "lr3.fit(X_train,y_train)\n",
    "lr3.coef_, lr3.intercept_\n",
    "# train_set_pred = lr3.predict(X3_train)\n",
    "# test_set_pred = lr3.predict(X3_test)\n",
    "# plt.scatter(test_set_pred1,y_test,alpha=.1)\n",
    "# plt.plot(np.linspace(0,600000,1000),np.linspace(0,600000,1000))\n",
    "# plt.scatter(y_test,y_test-test_set_pred1,alpha=.1)\n",
    "# plt.plot(np.linspace(0,600000,1000),np.linspace(0,0,1000))\n",
    "\n",
    "# plt.scatter(test_set_pred2,y_test,alpha=.1);\n",
    "# plt.plot(np.linspace(0,600000,1000),np.linspace(0,600000,1000));\n",
    "\n",
    "# plt.scatter(y_test, y_test-test_set_pred2,alpha=.1);\n",
    "# plt.plot(np.linspace(0,600000,1000),np.linspace(0,0,1000));\n",
    "\n",
    "\n",
    "# scores = cross_val_score(lr, X_train, y_train, cv=10, scoring='mean_squared_error')\n",
    "# print(-scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diagnostic_plots\n",
    "ols_model=sm.OLS(y3_train, sm.add_constant(X3_train)).fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(X3_train, y3_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping from Individual Game Pages\n",
    "Available data:\n",
    "    1. Genre\n",
    "    2. Single/Multiplayer\n",
    "    3. Number of Languages\n",
    "    4. Developer\n",
    "    5. Publisher\n",
    "    6. User-defined tages\n",
    "    7. Release date (repeat)\n",
    "    8. Recent Updates\n",
    "    9. System Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols2 = ['title', 'developer', 'user_tags', 'languages', 'genres', 'specs']\n",
    "df_gamepg=pd.DataFrame(columns=cols2, index=[])\n",
    "link_games=list(no_dup_df.link)\n",
    "\n",
    "for link in link_games[0:15]:\n",
    "    try:\n",
    "        response=requests.get(link)\n",
    "        soup = BeautifulSoup(response.text, 'html5lib')\n",
    "        row1={}\n",
    "        row1['title']=soup.find('div', class_='apphub_AppName').text\n",
    "        for developers in soup.find_all('div', class_=\"summary column\", id=\"developers_list\"):\n",
    "            developer=[one_developer.text for one_developer in developers.find_all('a')]\n",
    "        row1['developer']=developer\n",
    "        user_tags=[tag.text.replace(\"\\n\",\"\").strip(\"\\t\") for tag in soup.find_all('a', class_=\"app_tag\")]\n",
    "        row1['user_tags']=user_tags\n",
    "        all_lang=[]\n",
    "        for languages in soup.find_all('table', class_=\"game_language_options\"):\n",
    "            for element in languages.find_all('tr', style=True, class_=True):\n",
    "                language=(element.find('td', class_=\"ellipsis\").text.replace(\"\\n\",\"\").strip(\"\\t\"))\n",
    "                options=[language if bool(options.text.replace(\"\\n\",\"\").strip(\"\\t\")) else 0 for options in element.find_all('td', class_=\"checkcol\")]\n",
    "                all_lang.append(tuple(options))\n",
    "        row1['languages']=all_lang\n",
    "        x=soup.find_all('div', class_=\"details_block\")[0].text.replace(\"\\n\",\"\").split(\"\\t\")\n",
    "        feature_list = list(filter(None, x))\n",
    "        y=feature_list[1].split(\":\")\n",
    "        genres=y[1]\n",
    "        row1['genres']=genres\n",
    "        specs=[]\n",
    "        for element in soup.find_all('div', class_=\"game_area_details_specs\"):\n",
    "            l=[]\n",
    "            for e2 in element.find_all('a'):\n",
    "                l.append(e2.text.replace(\"\\n\",\"\").strip(\"\\t\"))\n",
    "            specs.extend(l)\n",
    "        full_specs=list(filter(None, specs))\n",
    "        row1['specs']=full_specs \n",
    "        df_gamepg=df_gamepg.append(row1, ignore_index=True)\n",
    "    except:\n",
    "        print(link)\n",
    "        \n",
    "df_gamepg.set_index('title', inplace=True)\n",
    "df_gamepg.rename(columns=lambda x: x.strip())\n",
    "pd.options.display.max_rows = 4000\n",
    "df_gamepg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_genres=[]\n",
    "for element in df_gamepg['genres']:\n",
    "    x=element.split(\",\")\n",
    "    num_genres.append(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single=[1 if 'Single-player' in element else 0 for element in df_gamepg['specs']]\n",
    "multi=[1 if 'Multi-player' in element else 0 for element in df_gamepg['specs']]\n",
    "controller_support=[1 if 'Full controller support' in element else 0 for element in df_gamepg['specs']]\n",
    "cards=[1 if 'Single-player' in element else 0 for element in df_gamepg['specs']]\n",
    "in_app_purchases=[1 if 'Single-player' in element or 'Partial Controller Support' else 0 for element in df_gamepg['specs']]\n",
    "cloud=[1 if 'Steam Cloud' in element else 0 for element in df_gamepg['specs']]\n",
    "workshop=[1 if 'Steam Workshop' in element else 0 for element in df_gamepg['specs']]\n",
    "captions=[1 if 'Captions available' in element else 0 for element in df_gamepg['specs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gamepg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_dup_gamepg_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
